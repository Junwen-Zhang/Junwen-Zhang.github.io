<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>面向分布式深度学习的数据中心网络拓扑优化和流量调度_王帅</title>
      <link href="/2023/03/18/%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87%E2%80%94%E2%80%94%E9%9D%A2%E5%90%91%E5%88%86%E5%B8%83%E5%BC%8F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91%E4%BC%98%E5%8C%96%E5%92%8C%E6%B5%81%E9%87%8F%E8%B0%83%E5%BA%A6-%E7%8E%8B%E5%B8%85/"/>
      <url>/2023/03/18/%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87%E2%80%94%E2%80%94%E9%9D%A2%E5%90%91%E5%88%86%E5%B8%83%E5%BC%8F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91%E4%BC%98%E5%8C%96%E5%92%8C%E6%B5%81%E9%87%8F%E8%B0%83%E5%BA%A6-%E7%8E%8B%E5%B8%85/</url>
      
        <content type="html"><![CDATA[<h1 id="论文阅读列表">论文阅读列表</h1><h2 id="基础">基础</h2><table><colgroup><col style="width: 13%"><col style="width: 14%"><col style="width: 72%"></colgroup><thead><tr class="header"><th>名称</th><th>简介</th><th>论文标题</th></tr></thead><tbody><tr class="odd"><td>RNN</td><td>循环神经网络</td><td></td></tr><tr class="even"><td>LSTM</td><td>长短记忆网络</td><td></td></tr><tr class="odd"><td>Transformer</td><td>自注意力网络</td><td>Attention is all you need</td></tr><tr class="even"><td>BERT</td><td>自注意力网络</td><td>BERT: Pre-training of deep bidirectional transformers for languageunderstanding</td></tr><tr class="odd"><td>GPT</td><td>自注意力网络</td><td>Language models are few-shot learners</td></tr></tbody></table><table><colgroup><col style="width: 14%"><col style="width: 22%"><col style="width: 62%"></colgroup><thead><tr class="header"><th>名称</th><th>简介</th><th>论文标题</th></tr></thead><tbody><tr class="odd"><td>BSP</td><td>整体同步并行</td><td>Geeps: Scalable deep learning on distributed gpus with agpu-specialized parameter server</td></tr><tr class="even"><td>PS</td><td>参数服务器架构</td><td>Scaling distributed machine learning with the parameter server</td></tr><tr class="odd"><td>Ring Allreduce</td><td>环全归约架构</td><td>https://github.com/baidu-research/baidu-allreduce</td></tr><tr class="even"><td>MPI</td><td>TensorFlow的集合通信库</td><td>https://www.open-mpi.org</td></tr><tr class="odd"><td>NCCL</td><td>Pytorch的集合通信库</td><td>https://developer.nvidia.com/nccl</td></tr><tr class="even"><td>Horovod</td><td>MXNet的集合通信库</td><td>https://horovod.ai</td></tr></tbody></table><table><colgroup><col style="width: 4%"><col style="width: 35%"><col style="width: 60%"></colgroup><thead><tr class="header"><th>名称</th><th>简介</th><th>论文标题</th></tr></thead><tbody><tr class="odd"><td>RoCE</td><td>RDMA overConverged Ethernet传输协议</td><td>RDMA over converged ethernet (RoCE)</td></tr><tr class="even"><td></td><td>Incast问题</td><td>The panasas activescale storage cluster-delivering scalable highbandwidth storage</td></tr></tbody></table><h2 id="针对通信数据量的优化">针对通信数据量的优化</h2><h3 id="并行方式优化">并行方式优化</h3><table><colgroup><col style="width: 16%"><col style="width: 19%"><col style="width: 64%"></colgroup><thead><tr class="header"><th>名称</th><th>简介</th><th>论文标题</th></tr></thead><tbody><tr class="odd"><td>Stanza</td><td>混合并行，算子拆分</td><td>Stanza: Layer separation for distributed training in deeplearning</td></tr><tr class="even"><td>Tofu</td><td></td><td>Supporting very large models using automatic dataflow graphpartitioning</td></tr><tr class="odd"><td>Hypar</td><td></td><td>Hypar: Towards hybrid parallelism for deep learning acceleratorarray</td></tr><tr class="even"><td>FlexFlow</td><td></td><td>Beyond data and model parallelism for deep neural networks</td></tr><tr class="odd"><td>Mesh-TensorFlow</td><td></td><td>Mesh-tensorflow: Deep learning for supercomputers</td></tr><tr class="even"><td>GPipe</td><td>微批次流水并行</td><td>Gpipe: Efficient training of giant neural networks using</td></tr><tr class="odd"><td>PipeDream</td><td>任务调度算法去1F1B</td><td>PipeDream: generalized pipeline parallelism for</td></tr><tr class="even"><td>DAPPLE</td><td></td><td>DAPPLE: A pipelined data parallel approach for training largemodels</td></tr><tr class="odd"><td>PipeMare</td><td></td><td>Pipemare: Asynchronous pipeline parallel DNN training</td></tr></tbody></table><h3 id="参数同步算法优化">参数同步算法优化</h3><table><colgroup><col style="width: 15%"><col style="width: 15%"><col style="width: 68%"></colgroup><thead><tr class="header"><th>名称</th><th>简介</th><th>论文标题</th></tr></thead><tbody><tr class="odd"><td>BSP</td><td>整体同步并行</td><td>Geeps: Scalable deep learning on distributed gpus with agpu-specialized parameter server</td></tr><tr class="even"><td>PS</td><td>参数服务器架构</td><td>Scaling distributed machine learning with the parameter server</td></tr><tr class="odd"><td>Ring Allreduce</td><td>环全归约架构</td><td>https://github.com/baidu-research/baidu-allreduce</td></tr></tbody></table><h3 id="通信内容压缩">通信内容压缩</h3><table><colgroup><col style="width: 4%"><col style="width: 28%"><col style="width: 67%"></colgroup><thead><tr class="header"><th>名称</th><th>简介</th><th>论文标题</th></tr></thead><tbody><tr class="odd"><td>QSGD</td><td>量化Quantization+误差补偿</td><td>QSGD: Randomized quantization for communication-optimal stochasticgradient descen</td></tr><tr class="even"><td></td><td></td><td>Mixed precision training</td></tr><tr class="odd"><td></td><td></td><td>1-bit stochastic gradient descent and its application to dataparallel distributed training of speech dnns</td></tr><tr class="even"><td></td><td>稀疏化Sparsification</td><td>Sparse online learning via truncated gradient</td></tr><tr class="odd"><td></td><td></td><td>Sparsified SGD with memory</td></tr><tr class="even"><td></td><td></td><td>Sparse communication for distributed gradient descent</td></tr><tr class="odd"><td></td><td></td><td>A distributed synchronous SGD algorithm with global top-ksparsification for low bandwidth networks</td></tr><tr class="even"><td></td><td>量化+稀疏化</td><td>Sparse binary compression: Towards distributed deep learning withminimal communication</td></tr></tbody></table><h2 id="针对通信次数的优化">针对通信次数的优化</h2><h3 id="异步模型训练">异步模型训练</h3><table><colgroup><col style="width: 6%"><col style="width: 26%"><col style="width: 66%"></colgroup><thead><tr class="header"><th>名称</th><th>简介</th><th>论文标题</th></tr></thead><tbody><tr class="odd"><td>ASP</td><td>异步并行</td><td>Hogwild!: A lock-free approach to parallelizing stochastic gradientdescent</td></tr><tr class="even"><td>SSP</td><td>延迟同步并行</td><td>More effective distributed ml via a stale synchronous parallelparameter server</td></tr><tr class="odd"><td>DynSSP</td><td>采用不同学习率优化收敛性</td><td>Heterogeneity-aware distributed parameter servers</td></tr><tr class="even"><td>Petuun</td><td>目前支持SSP的框架</td><td>Petuum: A new platform for distributed machine learning on big</td></tr></tbody></table><h3 id="调节批量大小">调节批量大小</h3><table><colgroup><col style="width: 5%"><col style="width: 20%"><col style="width: 74%"></colgroup><thead><tr class="header"><th>名称</th><th>简介</th><th>论文标题</th></tr></thead><tbody><tr class="odd"><td>LARS</td><td>逐层自适应学习率</td><td>Large batch training of convolutional networks</td></tr><tr class="even"><td></td><td></td><td>Accurate, large minibatch sgd: Training imagenet in 1 hour</td></tr><tr class="odd"><td>LAMB</td><td>在BERT上提高精度</td><td>Reducing BERT pre-training time from 3 days to 76 minutes</td></tr></tbody></table><h2 id="数据中心网络通信能力优化">数据中心网络通信能力优化</h2><h3 id="针对网络拓扑优化">针对网络拓扑优化</h3><table><colgroup><col style="width: 11%"><col style="width: 16%"><col style="width: 71%"></colgroup><thead><tr class="header"><th>名称</th><th>简介</th><th>论文标题</th></tr></thead><tbody><tr class="odd"><td>Fat-Tree</td><td>交换机为中心</td><td>A scalable, commodity data center network architecture</td></tr><tr class="even"><td>VL2</td><td></td><td>VL2: A scalable and flexible data center network</td></tr><tr class="odd"><td>Helios</td><td></td><td>Helios: a hybrid electrical/optical switch architecture for modulardata centers</td></tr><tr class="even"><td>c-Through</td><td></td><td>c-Through: Part-time optics in data centers</td></tr><tr class="odd"><td>Lesf-Spine</td><td></td><td>Cisco data center spine-and-leaf architecture: Design overview</td></tr><tr class="even"><td>BCube</td><td>以服务器为中心</td><td>BCube: a high performance, server-centric network architecture formodular data centers</td></tr><tr class="odd"><td>Dcell</td><td></td><td>DCell: a scalable and fault-tolerant network structure for datacenters</td></tr><tr class="even"><td>Fri-Conn</td><td></td><td>FiConn: Using backup port for server interconnection in datacenters</td></tr><tr class="odd"><td>MDCube</td><td></td><td>MDCube: a high performance network structure for modular data centerinterconnection</td></tr><tr class="even"><td>Torus</td><td></td><td>Blue gene/l torus interconnection network</td></tr></tbody></table><h3 id="针对传输协议优化">针对传输协议优化</h3><table><colgroup><col style="width: 8%"><col style="width: 41%"><col style="width: 50%"></colgroup><thead><tr class="header"><th>名称</th><th>简介</th><th>论文标题</th></tr></thead><tbody><tr class="odd"><td>InfiniBand</td><td>RDMA 技术的最早实现</td><td></td></tr><tr class="even"><td>RoCE</td><td>兼容以太网</td><td></td></tr><tr class="odd"><td>RoCEv2</td><td>IP 协议和 UDP 协议封装 IB 的传输层协议</td><td></td></tr><tr class="even"><td>PFC</td><td>基于优先级的流控机制</td><td>qbb-priority-based flow control</td></tr><tr class="odd"><td></td><td>拥塞控制，避免触发 PFC 暂停帧</td><td>Congestion control for large-scale RDMA deployments</td></tr><tr class="even"><td></td><td></td><td>TIMELY: RTT-based congestion control for the data center</td></tr><tr class="odd"><td></td><td></td><td>DCQCN+: Taming large-scale incast congestion in RDMA over ethernetnetworks</td></tr><tr class="even"><td></td><td></td><td>HPCC: High precision congestion control</td></tr><tr class="odd"><td></td><td>改进的 RoCEv2 网卡设计，降低触发 PFC 暂停帧的概率</td><td>Revisiting network support for RDMA</td></tr></tbody></table><h3 id="基于流量调度优化">基于流量调度优化</h3><figure><img src="/2023/03/18/%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87%E2%80%94%E2%80%94%E9%9D%A2%E5%90%91%E5%88%86%E5%B8%83%E5%BC%8F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91%E4%BC%98%E5%8C%96%E5%92%8C%E6%B5%81%E9%87%8F%E8%B0%83%E5%BA%A6-%E7%8E%8B%E5%B8%85/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E8%B0%83%E5%BA%A6%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C.png" alt="数据中心网络流量调度相关工作"><figcaption aria-hidden="true">数据中心网络流量调度相关工作</figcaption></figure><h2 id="分布式训练通信效率优化">分布式训练通信效率优化</h2><h3 id="针对小数据量通信优化">针对小数据量通信优化</h3><p>深度神经网络通常包含大量的小参数，但这些小参数在被传输时难以充分利用带宽资源</p><table><colgroup><col style="width: 8%"><col style="width: 22%"><col style="width: 68%"></colgroup><thead><tr class="header"><th>名称</th><th>简介</th><th>论文标题</th></tr></thead><tbody><tr class="odd"><td></td><td>horovod的张量融合</td><td>https://horovod.readthedocs.io/en/stable/tensor-fusion_in</td></tr><tr class="even"><td>MG-WFBP</td><td>求解最优张量融合方案</td><td>MG-WFBP: Efficient data communication for distributed synchronoussgd algorithms</td></tr></tbody></table><h3 id="针对通信次序的优化">针对通信次序的优化</h3><p>分布式训练需要传输大量的参数，并且传输次序具有随机性，导致紧急参数需要与其他参数竞争带宽资源。</p><table><colgroup><col style="width: 16%"><col style="width: 5%"><col style="width: 77%"></colgroup><thead><tr class="header"><th>名称</th><th>简介</th><th>论文标题</th></tr></thead><tbody><tr class="odd"><td>Tic-Tac</td><td></td><td>Tictac: Accelerating distributed deep learning with communicationscheduling</td></tr><tr class="even"><td>P3</td><td></td><td>Priority-based parameter propagation for distributed dnntraining</td></tr><tr class="odd"><td>ByteScheduler</td><td></td><td>A generic communication scheduler for distributed dnn trainingacceleration</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 学位论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式深度学习 </tag>
            
            <tag> 通信优化 </tag>
            
            <tag> 数据中心网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TimeLoop</title>
      <link href="/2023/03/16/TimeLoop/"/>
      <url>/2023/03/16/TimeLoop/</url>
      
        <content type="html"><![CDATA[<p>作用：映射神经网络，找到最优的数据重用工作流。</p><h1 id="原理">原理</h1><figure><img src="/2023/03/16/TimeLoop/image-20230316120500047.png" alt="image-20230316120500047"><figcaption aria-hidden="true">image-20230316120500047</figcaption></figure><h1 id="如何使用">如何使用</h1><h2 id="model-conv1d-1level">00-model-conv1d-1level</h2><p><img src="/2023/03/16/TimeLoop/image-20230215163226554.png" alt="image-20230215163226554" style="zoom: 25%;"></p><p>通过文件<code>timeloop-model.accelergy.log</code>可知，timeloop生成映射后会自动调用accelergy，将输入的architecture通过<code>catci</code>组件评估。</p><figure><img src="/2023/03/16/TimeLoop/image-20230215162913799.png" alt="image-20230215162913799"><figcaption aria-hidden="true">image-20230215162913799</figcaption></figure><p>输出的读写次数不一样，<strong>因为第一次做MAC运算不用读只需写回结果？？？</strong>。</p><figure><img src="/2023/03/16/TimeLoop/image-20230215161827393.png" alt="image-20230215161827393"><figcaption aria-hidden="true">image-20230215161827393</figcaption></figure><h2 id="model-conv1d-2level">01-model-conv1d-2level</h2><p><img src="/2023/03/16/TimeLoop/image-20230215164012799.png" alt="image-20230215164012799" style="zoom: 25%;"></p><p>定义了两种映射的方式，<code>output stationary</code>和<code>weight stationary</code>。</p><figure><img src="/2023/03/16/TimeLoop/image-20230215163758778.png" alt="image-20230215163758778"><figcaption aria-hidden="true">image-20230215163758778</figcaption></figure><p>按照step4改成 3 1/1 1920之后，ws会报错：</p><figure><img src="/2023/03/16/TimeLoop/image-20230215170843695.png" alt="image-20230215170843695"><figcaption aria-hidden="true">image-20230215170843695</figcaption></figure><p>改成 （2）3 64/1 30 后比<code>os</code>的 （1）1 1920/3 1对比如下，能耗变大了。原因是，在比较小的3*16的计算中，所有的计算数和计算结果都能存储在buffer中，而计算量变大后（1）仍然可以在内层buffer中完成全部计算，由于是OS，每次mac只有一个数需要重新取，而（2）不是完全的WS，没算完30个output，都会再取一遍weight，从而多了63*3=189次的从mainMemory到buffer的weight数据读取.。。。</p><figure><img src="/2023/03/16/TimeLoop/image-20230215171153214.png" alt="image-20230215171153214"><figcaption aria-hidden="true">image-20230215171153214</figcaption></figure><h2 id="model-conv1doutput-channels-2level">02-model-conv1d+outputchannels-2level</h2><p>讨论了分块问题（tail）<img src="/2023/03/16/TimeLoop/image-20230215175930634.png" alt="image-20230215175930634"></p><p>buffer层的区别，主要在Input多读了一次进来</p><figure><img src="/2023/03/16/TimeLoop/image-20230225144923476.png" alt="image-20230225144923476"><figcaption aria-hidden="true">image-20230225144923476</figcaption></figure><p>mainmemory的区别，也主要在input多读了一遍</p><figure><img src="/2023/03/16/TimeLoop/image-20230225145210890.png" alt="image-20230225145210890"><figcaption aria-hidden="true">image-20230225145210890</figcaption></figure><h2 id="model-conv1doc-3level">03-model-conv1d+oc-3level</h2><figure><img src="/2023/03/16/TimeLoop/image-20230215181203973.png" alt="image-20230215181203973"><figcaption aria-hidden="true">image-20230215181203973</figcaption></figure><p>讨论了bypass的问题，感觉这里bypass的作用其实是定义连接的方式，选择性地绕过某一个存储层，<strong>这里可以理解为RF直接和MM相联</strong>。</p><figure><img src="/2023/03/16/TimeLoop/image-20230225174724380.png" alt="image-20230225174724380"><figcaption aria-hidden="true">image-20230225174724380</figcaption></figure><p>RegisterFile层的bypass：output数据从Main Memory到RegisterFile虽然经过了ClobalBuffer，但没有存储在其中，因而相比没有bypass，节省了读取GlobalBuffer的能量，但网络传输的能量未发生变化。</p><p>GLB层的bypass：Weight和Input读的次数显著提升，但省去了output的读取。</p><figure><img src="/2023/03/16/TimeLoop/image-20230225174059773.png" alt="image-20230225174059773"><figcaption aria-hidden="true">image-20230225174059773</figcaption></figure><h2 id="model-conv1docic-3levelspatial">04-model-conv1d+oc+ic-3levelspatial</h2><figure><img src="/2023/03/16/TimeLoop/image-20230215181715931.png" alt="image-20230215181715931"><figcaption aria-hidden="true">image-20230215181715931</figcaption></figure><p>问题：[C, K, R, P]</p><p>Timeloop自动检测并利用多播机会。</p><p><code>KP</code>: Output 分块, Input数据被自动的多播（从GLB到RF）</p><p><code>CP</code>：Input 分块,Output数据被自动地多播（从GLB到RF），而Output数据存回时（从RF到GLB）可以先进行压缩再存回，与多播相反。观察输出文件，实际效果更差，因为分块了之后Output数据还要写回，而Input数据不用，所以这个策略能效较差。</p><h2 id="mapper-conv1doc-3level">05-mapper-conv1d+oc-3level</h2><p>2^(2*3) = 64 映射空间的大小？？？</p><p>mapper字段变成了下图所示，还有多了一个mapspace_constraints字段。</p><p><img src="/2023/03/16/TimeLoop/image-20230225220028110.png" alt="image-20230225220028110" style="zoom:50%;"></p><h2 id="mapper-convlayer-eyeriss">06-mapper-convlayer-eyeriss</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mapper:</span><br><span class="line">  optimization-metrics: [ delay, energy ]</span><br><span class="line">  live-status: False</span><br><span class="line">  num-threads: 8</span><br><span class="line">  timeout: 15000</span><br><span class="line">  victory-condition: 500</span><br><span class="line">  algorithm: random-pruned</span><br><span class="line">  max-permutations-per-if-visit: 16</span><br></pre></td></tr></table></figure><p>硬件架构和数据流的限制：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">architecture_constraints:</span><br><span class="line">  targets:</span><br><span class="line">  # certain buffer only stores certain datatypes</span><br><span class="line">  - target: psum_spad</span><br><span class="line">    type: bypass</span><br><span class="line">    bypass: [Inputs, Weights]</span><br><span class="line">    keep: [Outputs]</span><br><span class="line">  - target: weights_spad</span><br><span class="line">    type: bypass</span><br><span class="line">    bypass: [Inputs, Outputs]</span><br><span class="line">    keep: [Weights]</span><br><span class="line">  - target: ifmap_spad</span><br><span class="line">    type: bypass</span><br><span class="line">    bypass: [Weights, Outputs]</span><br><span class="line">    keep: [Inputs]</span><br><span class="line">  - target: DummyBuffer</span><br><span class="line">    type: bypass</span><br><span class="line">    bypass: [Inputs, Outputs, Weights]</span><br><span class="line">  - target: shared_glb</span><br><span class="line">    type: bypass</span><br><span class="line">    bypass: [Weights]</span><br><span class="line">    keep: [Inputs, Outputs]</span><br><span class="line">  - target: DummyBuffer</span><br><span class="line">    type: spatial</span><br><span class="line">    split: 4</span><br><span class="line">    permutation: NPQR SCM</span><br><span class="line">    factors: N=1 P=1 Q=1 R=1 S=0</span><br><span class="line">  # only allow fanout of M, Q out from glb</span><br><span class="line">  - target: shared_glb</span><br><span class="line">    type: spatial</span><br><span class="line">    split: 7</span><br><span class="line">    permutation: NCPRSQM</span><br><span class="line">    factors: N=1 C=1 P=1 R=1 S=1</span><br><span class="line">  # one ofmap position but of different output channels</span><br><span class="line">  - target: psum_spad</span><br><span class="line">    type: temporal</span><br><span class="line">    permutation: NCPQRS M</span><br><span class="line">    factors: N=1 C=1 R=1 S=1 P=1 Q=1</span><br><span class="line">  # row stationary -&gt; 1 row at a time</span><br><span class="line">  - target: weights_spad</span><br><span class="line">    type: temporal</span><br><span class="line">    permutation: NMPQS CR</span><br><span class="line">    factors: N=1 M=1 P=1 Q=1 S=1 R=0</span><br><span class="line">  - target: ifmap_spad</span><br><span class="line">    type: temporal</span><br><span class="line">    permutation: NMCPQRS</span><br><span class="line">    factors: N=1 M=1 C=1 P=1 Q=1 R=1 S=1</span><br><span class="line">  # enforce the hardware limit of the bypassing everything</span><br><span class="line">  - target: DummyBuffer</span><br><span class="line">    type: temporal</span><br><span class="line">    factors: N=1 M=1 C=1 P=1 Q=1 R=1 S=1</span><br></pre></td></tr></table></figure><p>下面的约束不是硬件架构和数据流的限制，而是帮助限制搜索空间以加快搜索速度.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">mapspace_constraints:</span><br><span class="line">  targets:</span><br><span class="line">    # intuitive optimization to reduce map space size</span><br><span class="line">    # the factors of these are 1 anyways, so the order does not really matter</span><br><span class="line">    - target: DummyBuffer</span><br><span class="line">      type: temporal</span><br><span class="line">      permutation: NMCPQRS</span><br><span class="line">    # intuitive optimization for row stationary</span><br><span class="line">    # -&gt; process a row/col of the output before going to the next one</span><br><span class="line">    - target: shared_glb</span><br><span class="line">      type: temporal</span><br><span class="line">      permutation: QRSC PNM</span><br><span class="line">      factors: Q=1 R=1 S=1 P=0</span><br><span class="line">    # intuitive optimization to reduce map space size</span><br><span class="line">    - target: DRAM</span><br><span class="line">      type: temporal</span><br><span class="line">      permutation: RSP CMNQ</span><br><span class="line">      factors: R=1 S=1 P=1</span><br></pre></td></tr></table></figure><h2 id="未理解的概念">未理解的概念</h2><p><code>timeloop-model.stats.txt</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Temporal reductions</span><br><span class="line">per-instance</span><br><span class="line">per-cluster</span><br><span class="line">Fanout</span><br><span class="line">Multicast factor</span><br><span class="line">Average number of hops : 0.50</span><br><span class="line">Spatial Reduction Energy</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 毕业设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模拟器 </tag>
            
            <tag> DNN加速器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparseLoop论文阅读笔记</title>
      <link href="/2023/03/15/SparseLoop%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/03/15/SparseLoop%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="问题分解表述">1 问题分解表述</h1><h2 id="一些概念">1.1 一些概念</h2><p>SAFs：sparse acceleration features（i.e. representation format,gating, and skipping）</p><p>IneffOps：ineffectual operations(i.e. arithmetic operations andstorage accesses associated with ineffectual computations)</p><h2 id="稀疏计算加速器特征">1.2 稀疏计算加速器特征</h2><p>High-Level Sparse Acceleration Features</p><h3 id="representation-format">1.2.1 Representation format</h3><p>只看一维的稀疏向量，常见的表示法</p><figure><img src="/2023/03/15/SparseLoop%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/%E4%B8%80%E7%BB%B4%E7%A8%80%E7%96%8F%E5%90%91%E9%87%8F%E7%9A%84%E5%B8%B8%E8%A7%81%E8%A1%A8%E7%A4%BA%E6%B3%95" alt="一维稀疏张量常见表示格式"><figcaption aria-hidden="true">一维稀疏张量常见表示格式</figcaption></figure><p>多维的稀疏张量，可以用单维表示法聚合</p><figure><img src="/2023/03/15/SparseLoop%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/%E5%A4%9A%E7%BB%B4%E7%A8%80%E7%96%8F%E5%BC%A0%E9%87%8F%E8%81%9A%E5%90%88%E8%A1%A8%E7%A4%BA" alt="多维稀疏张量的聚合表示"><figcaption aria-hidden="true">多维稀疏张量的聚合表示</figcaption></figure><h3 id="gating-ineffops">1.2.2 Gating IneffOps</h3><p>可应用于存储单元和计算单元，让其保持空闲；可以节省能耗，但不改变处理周期。</p><p>A、先看单个操作数（leader），如果是0，则不访问另一个操作数（follower）的地址；因此不会消除全部的IneffOps，取决于leader的稀疏性特征。GateFollower ← Leader</p><p>B、两个操作数同时看判断是否都不为0，通常靠的是存储格式的元数据；可以消除所有的IneffOps，比A的硬件复杂。Operand0↔︎Operand 1</p><figure><img src="/2023/03/15/SparseLoop%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/gating%E5%92%8Cskipping%E7%A4%BA%E4%BE%8B" alt="Gating和skipping示例"><figcaption aria-hidden="true">Gating和skipping示例</figcaption></figure><h3 id="skipping-ineffops">1.2.3 Skipping IneffOps</h3><p>直接跳过，进入下一个有效计算；同时节省能耗和处理周期。</p><p>将跳过应用于计算单元时，计算单元会直接查找下一对操作数，直到找到要执行的有效计算。当跳跃应用于存储单元时，它也可以通过查看单个操作数或两个操作数同时查看（同（2）），从而找到下一次有效计算。</p><p>比门控的硬件更复杂。</p><h2 id="数据流">1.3 数据流</h2><p>DNN/矩阵乘法。</p><p>数据流定义时间和空间中数据移动和计算的计划，SAF定义实际移动的数据量或执行的计算数。因此，稀疏张量加速器的设计空间是数据流选择和SAF 选择的交叉乘积</p><figure><img src="/2023/03/15/SparseLoop%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/%E4%B8%8D%E5%90%8C%E5%8A%A0%E9%80%9F%E5%99%A8%E6%9E%B6%E6%9E%84%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81%E5%92%8CSAFs" alt="经典加速器的workload和SAFs描述"><figcaption aria-hidden="true">经典加速器的workload和SAFs描述</figcaption></figure><p>上图的这种分类法，可以将稀疏计算加速器系统地描述，并可用于定性地比较。</p><h1 id="sparseloop概述">2 SparseLoop概述</h1><h2 id="设计挑战">2.1 设计挑战</h2><p>（1）设计空间非常大，SAFs和workload的乘积。</p><p>（2）精度和速度的平衡，比如同一个workload的映射空间非常大无法穷举，而启发式搜索可能不是最优解。</p><p>（3）SAFs设计/workload的发展</p><h2 id="解决方案">2.2 解决方案</h2><p>（1）对不同的设计方面进行解耦（dataflow、SAFs、微架构）</p><p>（2）一些复杂的相关性行为，采用统计数据建模。</p><figure><img src="/2023/03/15/SparseLoop%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/SparseLoop%E9%AB%98%E7%BA%A7%E6%A1%86%E6%9E%B6" alt="SparseLoop总体框架"><figcaption aria-hidden="true">SparseLoop总体框架</figcaption></figure><h1 id="稀疏循环框架">3 稀疏循环框架</h1><h2 id="输入">3.1 输入</h2><figure><img src="/2023/03/15/SparseLoop%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/SparseLoop%E8%BE%93%E5%85%A5.png" alt="SparseLoop的输入"><figcaption aria-hidden="true">SparseLoop的输入</figcaption></figure><h2 id="step1dataflow-modeling">3.2 STEP1：dataflow modeling</h2><p>继承自TimeLoop（一个DNN加速器的模拟器），主要的思想是，抽象出数据流在各个存储级别中的划分和数据移动，然后根据输入的硬件参数估算出能耗等指标。</p><h2 id="step2sparse-modeling">3.3 STEP2：Sparse modeling</h2><figure><img src="/2023/03/15/SparseLoop%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/sparse_modeling%E6%A1%86%E6%9E%B6.png" alt="sparse modeling总体框架"><figcaption aria-hidden="true">sparse modeling总体框架</figcaption></figure><p>稀疏建模首先使用特定于 SAF的分析器（即门控跳过分析器和格式分析器）在本地评估 SAF对每瓦流量的影响，然后通过简单的缩放对本地流量进行后处理，以反映 SAF对整体流量的影响。</p><ol type="1"><li><p>与格式无关的张量说明</p><figure><img src="/2023/03/15/SparseLoop%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/fibertree%E7%A4%BA%E4%BE%8B.png" alt="fibertree示例"><figcaption aria-hidden="true">fibertree示例</figcaption></figure><p><em>fibertree</em>：每一层（level/rank）是张量的一个秩，最后一级是非零值，准确地反映了张量的稀疏性特征。</p><p>每一个fiber对应于正在处理的tile</p></li><li><p>统计密度模型</p><p>不可能枚举每一个fiber的映射空间来探索最佳的设计，因此采用统计模型。对于同样的密度模型（6/16=0.375），同一级不同纤维之间的密度有很大差异（rankO中的纤维密度为 50%，概率为 0.75，密度为 0%，概率为 0.25）。</p><figure><img src="/2023/03/15/SparseLoop%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20230315215207742.png" alt="50% 随机分布的非零的张量中，各种形状fiber的密度概率。"><figcaption aria-hidden="true">50%随机分布的非零的张量中，各种形状fiber的密度概率。</figcaption></figure><p>密度模型分为两种，一种是不同维密度分布相同，另一种是密度分布不同。SparseLoop目前支持四种密度模型。</p><figure><img src="/2023/03/15/SparseLoop%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20230315215732835.png" alt="目前支持的四种密度模型，新模型可以自己添加"><figcaption aria-hidden="true">目前支持的四种密度模型，新模型可以自己添加</figcaption></figure></li><li><p>format分析</p><p>Sparseloop支持5种per-rank format models: <em>B, CP, UOP, RLE</em>,and <em>Uncompressed B</em>。 <span class="math display">\[Overhead_{RLE} = nonEmptyelEments × runLengthBitwidth \\Overhead_B = totalElements × 1\]</span></p></li><li><p>gating/skipping分析</p><p>原始的每块密集流量分解为三种细粒度操作类型：i） 实际发生，ii）跳过，iii） 被门控。</p></li><li><p>后处理</p><p>在分析器根据每个瓦的流量评估其各自 SAF的影响后，稀疏建模执行后处理，首先反映 SAF 之间的交互（例如，由于跳过SAF而跳过了多少格式开销），然后根据传输的瓦片数量缩放每个瓦的细分，以得出最终的稀疏流量。</p></li></ol><h2 id="step3arch-modeling">3.4 STEP3：Arch modeling</h2><p>对于能源消耗，我们使用能量估算后端（<em>例如</em>，Accelergy）来评估每个细粒度操作的成本，并将其与其相应的稀疏流量相结合以得出准确的能耗。</p><h1 id="评估">4 评估</h1>]]></content>
      
      
      <categories>
          
          <category> 组会论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 稀疏计算 </tag>
            
            <tag> 模拟器 </tag>
            
            <tag> DNN加速器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>accelergy原理和代码</title>
      <link href="/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/"/>
      <url>/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<p>能量估算公式：</p><p><span class="math display">\[Power = \alpha C V_{DD}^2 f\]</span></p><p>将造成每动作能量值差异的主要原因分为四类:</p><p>(1)动作特性</p><p>(2)数据特性</p><p>(3)时钟门控</p><p>(4)特定设计的优化</p><p>底层能量主要通过两个插件进行分析：CACTI和Aladdin。</p><h2 id="cacti">CACTI</h2><p>专门针对SRAM的能量估算</p><figure><img src="/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/CACTI.png" alt="image-20230310155149259"><figcaption aria-hidden="true">image-20230310155149259</figcaption></figure><h2 id="aladdin">Aladdin</h2><p>datapath（包括循环迭代并行性、流水线、数组分区和时钟频率，aladdin）+memory（缓存层次结构，DRAMSim2）</p><figure><img src="/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230310155559855.png" alt="image-20230310155559855"><figcaption aria-hidden="true">image-20230310155559855</figcaption></figure><h2 id="环境配置">环境配置</h2><p>1、安装anaconda，python&gt;=3.8</p><p>2、在<code>accelergy</code>文件夹下执行<code>pip install .</code>，自动地安装</p><p>3、发现<code>accelergy</code>被自动安装到了<code>\home\june\.lacal\bin</code>中，因此我们在<code>\home\june\.bashrc</code>文件中添加一句</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="string">&quot;<span class="variable">$PATH</span>:/home/june/.local/bin&quot;</span></span><br></pre></td></tr></table></figure><p>然后再<code>source .bashrc</code>激活配置</p><p>4、然后<code>acecerlgy</code>就成了可执行的命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> examples/hierarchy/input</span><br><span class="line">accelergy -o ../output/ *.yaml components/*.yaml -v 1</span><br></pre></td></tr></table></figure><h1 id="accelergy">accelergy</h1><ul><li>使用 <strong>Timeloop 映射探索</strong>，Timeloop 使用 DNN加速器的关键架构和实现属性的简洁统一表示来描述广泛的硬件架构空间。在精确能量估算器的帮助下，Timeloop通过映射器为任何给定工作负载生成准确的处理速度和能效特征，该映射器找到在指定架构上安排操作和暂存数据的最佳方式。</li><li>使用<strong>Accelergy能量估算</strong>，Accelergy作为能量估算器，提供灵活的能量估算，以促进Timeloop的能量表征。Accelergy允许任意加速器架构设计规范，这些设计由用户定义的特定于设计的高级复合组件和用户定义的低级基元组件组成，这些组件可以通过<strong>第三方能量估算插件</strong>进行表征，以反映设计的技术相关特征。</li></ul><h2 id="estimater">estimater</h2><figure><img src="/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230213161912908.png" alt="image-20230213161912908"><figcaption aria-hidden="true">image-20230213161912908</figcaption></figure><p>estimation_plug_ins/accelergy-aladdin-plug-in/aladdin.estimator.yaml</p><p>estimation_plug_ins/accelergy-cacti-plug-in/cactimator.yaml</p><p>estimation_plug_ins/accelergy-table-based-plug-ins/table.estimator.yaml</p><h2 id="architecture---eyeriss---v0.2">architecture - eyeriss -v0.2</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">1 smartbuffer_SRAM:</span></span><br><span class="line"><span class="string">eyeriss_like.weights_glb</span></span><br><span class="line"><span class="string">eyeriss_like.shared_glb</span></span><br><span class="line"></span><br><span class="line"><span class="attr">2 XY_NoC:</span></span><br><span class="line"><span class="string">eyeriss_like.weights_NoC</span></span><br><span class="line"><span class="string">eyeriss_like.ifmap_NoC</span></span><br><span class="line"><span class="string">eyeriss_like.psum_write_NoC</span></span><br><span class="line"><span class="string">eyeriss_like.psum_read_NoC</span></span><br><span class="line"></span><br><span class="line"><span class="attr">3 smartbuffer_RF:</span></span><br><span class="line"><span class="string">eyeriss_like.PE[0..167].weights_spad</span></span><br><span class="line"><span class="string">eyeriss_like.PE[0..167].ifmap_spad</span></span><br><span class="line"><span class="string">eyeriss_like.PE[0..167].psum_spad</span></span><br><span class="line"></span><br><span class="line"><span class="attr">4 intmac:</span></span><br><span class="line"><span class="string">eyeriss_like.PE[0..167].mac</span></span><br></pre></td></tr></table></figure><p>ation没出现PE[139], PE[125], PE[111], PE[97],PE[83],PE[69], PE[55],PE[41], PE[27], PE[13]，也就是12*14的最后一列。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="number">0.3</span></span><br><span class="line">  <span class="attr">subtree:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">eyeriss_like</span></span><br><span class="line">      <span class="attr">attributes:</span></span><br><span class="line">        <span class="attr">technology:</span> <span class="string">40nm</span></span><br><span class="line">      <span class="attr">local:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">weights_glb</span></span><br><span class="line">          <span class="attr">class:</span> <span class="string">smartbuffer_SRAM</span></span><br><span class="line">          <span class="attr">attributes:</span></span><br><span class="line">            <span class="attr">memory_width:</span> <span class="number">64</span></span><br><span class="line">            <span class="attr">memory_depth:</span> <span class="number">1024</span></span><br><span class="line">            <span class="attr">n_banks:</span> <span class="number">2</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">shared_glb</span></span><br><span class="line">          <span class="attr">class:</span> <span class="string">smartbuffer_SRAM</span></span><br><span class="line">          <span class="attr">attributes:</span></span><br><span class="line">            <span class="attr">memory_width:</span> <span class="number">64</span></span><br><span class="line">            <span class="attr">n_banks:</span> <span class="number">25</span></span><br><span class="line">            <span class="attr">bank_depth:</span> <span class="number">512</span></span><br><span class="line">            <span class="attr">memory_depth:</span> <span class="string">bank_depth</span> <span class="string">*</span> <span class="string">n_banks</span></span><br><span class="line">            <span class="attr">n_buffets:</span> <span class="number">2</span></span><br><span class="line">            <span class="attr">update_fifo_depth:</span> <span class="number">2</span> </span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ifmap_NoC</span></span><br><span class="line">          <span class="attr">class:</span> <span class="string">XY_NoC</span></span><br><span class="line">          <span class="attr">attributes:</span></span><br><span class="line">            <span class="attr">datawidth:</span> <span class="number">16</span> <span class="comment"># 输入的图像是16位低精度</span></span><br><span class="line">            <span class="attr">col_id_width:</span> <span class="number">5</span> </span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">weights_NoC</span></span><br><span class="line">          <span class="attr">class:</span> <span class="string">XY_NoC</span></span><br><span class="line">          <span class="attr">attributes:</span></span><br><span class="line">            <span class="attr">datawidth:</span> <span class="number">64</span> <span class="comment"># 权重是64位高精度</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">psum_write_NoC</span></span><br><span class="line">          <span class="attr">class:</span> <span class="string">XY_NoC</span></span><br><span class="line">          <span class="attr">attributes:</span></span><br><span class="line">            <span class="attr">datawidth:</span> <span class="number">64</span> <span class="comment"># 累加和是64位高精度</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">psum_read_NoC</span></span><br><span class="line">          <span class="attr">class:</span> <span class="string">XY_NoC</span></span><br><span class="line">          <span class="attr">attributes:</span></span><br><span class="line">            <span class="attr">datawidth:</span> <span class="number">64</span></span><br><span class="line">            <span class="attr">Y_X_wire_avg_length:</span> <span class="string">4mm</span></span><br><span class="line">      <span class="attr">subtree:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PE[0..167]</span></span><br><span class="line">        <span class="attr">attributes:</span></span><br><span class="line">          <span class="attr">memory_width:</span> <span class="number">16</span></span><br><span class="line">        <span class="attr">local:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ifmap_spad</span> <span class="comment">#存放输入</span></span><br><span class="line">            <span class="attr">class:</span> <span class="string">smartbuffer_RF</span></span><br><span class="line">            <span class="attr">attributes:</span></span><br><span class="line">              <span class="attr">memory_depth:</span> <span class="number">12</span> <span class="comment"># ???</span></span><br><span class="line">              <span class="attr">buffet_manager_depth:</span> <span class="number">0</span> </span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">weights_spad</span> <span class="comment">#存放权重</span></span><br><span class="line">            <span class="attr">class:</span> <span class="string">smartbuffer_SRAM</span> <span class="comment">#SRAM???</span></span><br><span class="line">            <span class="attr">attributes:</span></span><br><span class="line">              <span class="attr">memory_depth:</span> <span class="number">224</span> <span class="comment"># ??? 14*16</span></span><br><span class="line">              <span class="attr">buffet_manager_depth:</span> <span class="number">0</span> </span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">psum_spad</span> <span class="comment">#存放和</span></span><br><span class="line">            <span class="attr">class:</span> <span class="string">smartbuffer_RF</span></span><br><span class="line">            <span class="attr">attributes:</span></span><br><span class="line">              <span class="attr">memory_depth:</span> <span class="number">24</span> <span class="comment"># ???</span></span><br><span class="line">              <span class="attr">buffet_manager_depth:</span> <span class="number">24</span> <span class="comment"># 感觉就是scoreboard_depth</span></span><br><span class="line">              <span class="attr">update_fifo_depth:</span> <span class="number">2</span> </span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mac</span></span><br><span class="line">            <span class="attr">class:</span> <span class="string">intmac</span></span><br><span class="line">            <span class="attr">attributes:</span></span><br><span class="line">              <span class="attr">datawidth:</span> <span class="number">16</span></span><br></pre></td></tr></table></figure><h2 id="component---eyeriss---v0.3">component - eyeriss - v0.3</h2><h3 id="buffet感觉是用来数据索引的"><strong>buffet：感觉是用来数据索引的</strong></h3><p>scoreboard(regfile)+data_FIFO*2+addr_FIFO*2</p><p>scoreboard_depth等于depth，</p><p><img src="/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230113141445753.png" alt="image-20230113141445753" style="zoom:50%;"></p><blockquote><p>fill / read / update / idle</p></blockquote><p>来自action_count的参数：<code>address_delta</code>，<code>data_delta</code></p><h3 id="buffet_collection_rf"><strong>buffet_collection_RF:</strong></h3><p>buffet*n+ storage(regfile)</p><blockquote><p>fill / read / update / idle</p></blockquote><p><img src="/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230113143723978.png" alt="image-20230113143723978" style="zoom:50%;"></p><h3 id="buffet_collection_sram"><strong>buffet_collection_SRAM:</strong></h3><p>buffet*n+ storage(SRAM)</p><blockquote><p>fill / read / update / idle</p></blockquote><p><img src="/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230113143300014.png" alt="image-20230113143300014" style="zoom:50%;"></p><h3 id="smartbuffer_rf">smartbuffer_RF</h3><p>buffet_collection(buffet_collection_RF) +address_generators(counter)*2</p><p><img src="/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230113144701232.png" alt="image-20230113144701232" style="zoom:50%;"></p><blockquote><p>fill / read / update / idle</p></blockquote><h3 id="smartbuffer_sram">smartbuffer_SRAM</h3><p>buffet_collection(buffet_collection_SRAM) +address_generators(counter)*2</p><p><img src="/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230113144642130.png" alt="image-20230113144642130" style="zoom:50%;"></p><blockquote><p>fill / read / update / idle</p></blockquote><h3 id="xy_noc">XY_NoC</h3><p>Y_memory_controller[0..n_PE_rows-1](comparator) +X_memory_controller[0..total_PEs-1](comparator) + Y_X_wire(wire)</p><p><img src="/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230113144925605.png" alt="image-20230113144925605" style="zoom:50%;"></p><blockquote><p>transfer_random / transfer_repeated / idle</p></blockquote><h2 id="action-count">action count</h2><p>问题记录：</p><p>1、action里的参数address_delta，data_delta是什么？(发现在PE里address_delta只出现0，1两种取值，data_delta全程只出现0，1两种取值)</p><p>2、NoC的action里的参数n_cols_per_row，n_rows是什么？</p><p>3、为什么weights_spad是smartbuffer_SRAM，而其他的是smartbuffer_RF？</p><p>4、3个spad里（ifmap,weights,psum）的buffet_manager_depth为啥设成0、0、24？memory_depth设成12，224，24？</p><p>5、<img src="/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230113163005593.png" alt="image-20230113163005593" style="zoom:50%;"></p><p>这4种运算代表啥？mac_gated应该就是zerogating的情况，idle是不工作，random是最耗电的，reused是重复上一次计算结果。</p><h2 id="pim">PIM</h2><h3 id="输入文件分析">输入文件分析</h3><p>基于内存（忆阻器）的一个体系结构，输入文件中没有<code>action_counts.yaml</code>，输入的是<code>PIM_estimation_tables</code>文件夹。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelergyTables -r ./05_pim/PIM_estimation_tables</span><br></pre></td></tr></table></figure><figure><img src="/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230213162512868.png" alt="image-20230213162512868"><figcaption aria-hidden="true">image-20230213162512868</figcaption></figure><p><img src="/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230213163338902.png" alt="image-20230213163338902" style="zoom:67%;"></p><p>其中<code>32nm</code>文件夹存放传统的组件，有一部分已在安装的插件代码中出现过（比如DRAM.csv），有一部分是新定义的组件（如ADV.csv）。</p><p><img src="/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230213162942361.png" alt="image-20230213162942361" style="zoom:67%;"></p><p><code>memristor</code>文件夹存放基于忆阻器的组件。</p><p><img src="/2023/03/14/accelergy%E5%8E%9F%E7%90%86%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230213163624720.png" alt="image-20230213163624720" style="zoom:67%;"></p><h3 id="输出文件分析">输出文件分析</h3><p>没有<code>energy_estimation.yaml</code>，该文件用于评估原始组件的能量。其他输出文件都是有的。</p>]]></content>
      
      
      <categories>
          
          <category> 毕业设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模拟器 </tag>
            
            <tag> DNN加速器 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
